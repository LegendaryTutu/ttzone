{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  age  age_group  ed  employ  address  income  debtinc  creddebt  \\\n",
      "0   1   41          3   3      17       12    35.9     11.9  0.504108   \n",
      "\n",
      "    othdebt  default  \n",
      "0  3.767992        0  \n"
     ]
    }
   ],
   "source": [
    "#导入包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "df=pd.read_excel(\"C:\\\\Users\\\\有福有德\\\\Desktop\\\\上海统计分析python20200418\\\\data\\\\bankloan_binning.xlsx\")\n",
    "print(df.head(1))\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(df.iloc[:,[2,3,4,5,6,7,8,9]],df.iloc[:,-1]\n",
    "                                           ,test_size=0.2,random_state=0)#train_size=0.8\n",
    "xtrain1,xvalid,ytrain1,yvalid=train_test_split(xtrain,ytrain\n",
    "                                           ,test_size=0.2,random_state=0)#train_size=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         非违约       0.86      0.90      0.88       767\n",
      "          违约       0.60      0.50      0.54       233\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.73      0.70      0.71      1000\n",
      "weighted avg       0.80      0.81      0.80      1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.08962603,  0.21865473, -6.24846841, -2.99577209, -2.05988285,\n",
       "         -1.24658977, 29.37878788, 10.4398124 ]]), array([42.91018621]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#================1.python数据处理标准流程===============\n",
    "#第一、导入包和对应的类\n",
    "#第二、实例化\n",
    "#第三、拟合数据\n",
    "#第四、评估模型\n",
    "#第五、预测评分\n",
    "\n",
    "#分类预测\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf=SGDClassifier(loss='log',random_state=123)\n",
    "sgd_clf.fit(xtrain,ytrain)#拟合训练集数据\n",
    "sgd_clf.score(xtest,ytest)#非监督模型是transform\n",
    "y_sgd=sgd_clf.predict(xtest)#predict_proba\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest,y_sgd,target_names=['非违约','违约']))\n",
    "\n",
    "sgd_clf.coef_,sgd_clf.intercept_\n",
    "# 老(新)样本预测\n",
    "# x1=np.array([[2,1,3.767992,3.90,0.504108,3.767992,2.90,1.504108],\n",
    "#              [3,4,0.767992,3.90,1.504108,3.767992,0.90,1.504108]])\n",
    "# pdata=pd.DataFrame(sgd_clf.predict_proba(x1))#predict_proba需要logsitic回归\n",
    "# pdata.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#特点：1.SGD允许minibatch(在线/核外oob)学习，使用partial_fit方法；2.拟合大型列和行；\n",
      "      3.稀疏数据处理(loss参数和罚值控制),4.SGDClassifier支持多分类，依”one-vs-all”的形式\n",
      "#损失函数：\n",
      "    loss=”hinge”: (soft-margin)线性svm；\n",
      "    loss=”modified_huber”:稳健的异常值处理；\n",
      "    loss=”log”:logistic回归\n",
      "    loss=”perceptron”:感知器算法\n",
      "    其他损失函数如回归张的'huber', 'epsilon_insensitive'\n",
      "#惩罚项（或正则化）：l1与elasticnet可用于稀疏数据\n",
      "    penalty=”l2”: 对coef_的L2范数罚项；\n",
      "    penalty=”l1”: L1范数罚项；\n",
      "    penalty=”elasticnet”: L1与L2的convex组合；\n",
      "#alpha：乘以正则项的常数或变量（最优化算法）；\n",
      "#l1_ratio:弹性网混合参数，默认为0.15。取值[0,1],l1_ratio=0为L2，l1_ratio=1则为L1，注：(1-l1_ratio)*L2+l1_ratio*L1\n",
      "#max_iter: int,可选(默认=1000)：遍历训练数据的最大值(又名epochs)。只影响fit(),对partial_fit无效；\n",
      "#shuffle : bool，默认值为True,是否在每次epoch后随机打乱训练数据（洗牌）。\n",
      "#epsilon : float，如果loss='huber'或 'epsilon_insensitive'或 'squared_epsilon_insensitive'时可用；\n",
      "        如果预测和观测值间的差值小于此阈值，则忽略，即异常值修正参数；\n",
      "#learning_rate : string,默认'optimal'；\n",
      "    learning_rate='constant':eta = eta0，注eta0为初始学习率；\n",
      "    learning_rate='optimal':eta = 1.0 / (alpha * (t + t0))，\n",
      "    learning_rate='invscaling':eta = eta0 / pow(t, power_t)，注power_t选项另外指定；\n",
      "    learning_rate='adaptive':如果误差持续下降，则eta = eta0，否则（n_iter_no_change等参数满足）学习率除以5；\n",
      "#validation_fraction : float, default=0.1，验证集比例；\n",
      "#warm_start : bool,默认False，如果True,调用之前的解决拟合值作为初始化,否则清除；\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#==================2.模型参数===================\n",
    "sgd_clf=SGDClassifier(loss='hinge',\n",
    "                      penalty='l2',\n",
    "                      alpha=0.0001,\n",
    "                      l1_ratio=0.15,\n",
    "                      fit_intercept=True,\n",
    "                      max_iter=1000,\n",
    "                      tol=0.001,\n",
    "                      shuffle=True,\n",
    "                      verbose=0,\n",
    "                      epsilon=0.1,\n",
    "                      n_jobs=None,\n",
    "                      random_state=None,\n",
    "                      learning_rate='optimal',\n",
    "                      eta0=0.0,\n",
    "                      power_t=0.5,\n",
    "                      early_stopping=False,\n",
    "                      validation_fraction=0.1,\n",
    "                      n_iter_no_change=5,\n",
    "                      class_weight=None,\n",
    "                      warm_start=False,\n",
    "                      average=False,                 \n",
    "                     )            \n",
    "print(\"\"\"\n",
    "#特点：1.SGD允许minibatch(在线/核外oob)学习，使用partial_fit方法；2.拟合大型列和行；\n",
    "      3.稀疏数据处理(loss参数和罚值控制),4.SGDClassifier支持多分类，依”one-vs-all”的形式\n",
    "#损失函数：\n",
    "    loss=”hinge”: (soft-margin)线性svm；\n",
    "    loss=”modified_huber”:稳健的异常值处理；\n",
    "    loss=”log”:logistic回归\n",
    "    loss=”perceptron”:感知器算法\n",
    "    其他损失函数如回归张的'huber', 'epsilon_insensitive'\n",
    "#惩罚项（或正则化）：l1与elasticnet可用于稀疏数据\n",
    "    penalty=”l2”: 对coef_的L2范数罚项；\n",
    "    penalty=”l1”: L1范数罚项；\n",
    "    penalty=”elasticnet”: L1与L2的convex组合；\n",
    "#alpha：乘以正则项的常数或变量（最优化算法）；\n",
    "#l1_ratio:弹性网混合参数，默认为0.15。取值[0,1],l1_ratio=0为L2，l1_ratio=1则为L1，注：(1-l1_ratio)*L2+l1_ratio*L1\n",
    "#max_iter: int,可选(默认=1000)：遍历训练数据的最大值(又名epochs)。只影响fit(),对partial_fit无效；\n",
    "#shuffle : bool，默认值为True,是否在每次epoch后随机打乱训练数据（洗牌）。\n",
    "#epsilon : float，如果loss='huber'或 'epsilon_insensitive'或 'squared_epsilon_insensitive'时可用；\n",
    "        如果预测和观测值间的差值小于此阈值，则忽略，即异常值修正参数；\n",
    "#learning_rate : string,默认'optimal'；\n",
    "    learning_rate='constant':eta = eta0，注eta0为初始学习率；\n",
    "    learning_rate='optimal':eta = 1.0 / (alpha * (t + t0))，\n",
    "    learning_rate='invscaling':eta = eta0 / pow(t, power_t)，注power_t选项另外指定；\n",
    "    learning_rate='adaptive':如果误差持续下降，则eta = eta0，否则（n_iter_no_change等参数满足）学习率除以5；\n",
    "#validation_fraction : float, default=0.1，验证集比例；\n",
    "#warm_start : bool,默认False，如果True,调用之前的解决拟合值作为初始化,否则清除；\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
